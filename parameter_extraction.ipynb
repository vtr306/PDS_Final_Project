{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mv8f0trrHwhW"
   },
   "source": [
    "# Bibliotecas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eyx1cfeCHvst"
   },
   "outputs": [],
   "source": [
    "from scipy.signal import convolve\n",
    "from scipy.signal import hilbert\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.signal import correlate\n",
    "from scipy.signal import filtfilt\n",
    "from statistics import median_high\n",
    "from statistics import median_low\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importando Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(os.getcwd(), 'data_training')\n",
    "\n",
    "mat_files = []\n",
    "for file in os.listdir('data_training'):\n",
    "    if file.endswith(\".mat\"):\n",
    "        mat_files.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ECG_data = []\n",
    "for file in mat_files:\n",
    "    if os.name == 'nt':\n",
    "        bar = '\\\\'\n",
    "    else:\n",
    "        bar = '/'\n",
    "    current_path = path + bar + file\n",
    "    mat_data = scipy.io.loadmat(current_path)\n",
    "    ECG_data.append(mat_data['val'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtro de Linha de Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_filter(data, f_samp, n):\n",
    "    hi_D = [-0.2304, 0.7148, -0.6309, -0.0280, 0.1870, 0.0308, -0.0329, -0.0106]\n",
    "    lo_D = [-0.0106, 0.0329, 0.0308, -0.1870, -0.0280, 0.6309, 0.7148, 0.2304]\n",
    "    hi_R = [-0.0106, -0.0329, 0.0308, 0.1870, -0.0280, -0.6309, 0.7148, -0.2304]\n",
    "    lo_R = [0.2304, 0.7148, 0.6309, -0.0280, -0.1870, 0.0308, 0.0329, -0.0106]\n",
    "\n",
    "    cA, cD, A_list = decomposition_signal(lo_D, hi_D, data, f_samp, n)\n",
    "    cA = recomposition_signal(A_list, lo_R, hi_R, cA, cD, f_samp, n)\n",
    "\n",
    "    filtered_data = data - cA\n",
    "\n",
    "    return filtered_data\n",
    "\n",
    "def decomposition_signal(lo_D, hi_D, cA, f_samp, n):\n",
    "    A_list = []\n",
    "    for i in range(0, n):\n",
    "        # Convolução sem deslocamento do sinal e extensão do sinal no ínicio e no fim de f_samp amostras\n",
    "        A = 0.5*filtfilt(lo_D, 1, cA, padlen=0)\n",
    "        A = np.concatenate((A[0]*np.ones((1,f_samp))[0], A, A[-1]*np.ones((1,f_samp))[0]), axis=None)\n",
    "        A_list.append(A)\n",
    "        D = 0.5*filtfilt(hi_D, 1, cA, padlen=0)\n",
    "        D = np.concatenate((D[0]*np.ones((1,f_samp))[0], D, D[-1]*np.ones((1,f_samp))[0]), axis=None)\n",
    "        # Subamostragem do Sinal excluíndo metade das amostras\n",
    "        cA = [x for index, x in enumerate(A) if index%2 == 0]\n",
    "        cD = [x for index, x in enumerate(D) if index%2 == 0]\n",
    "\n",
    "    return cA, cD, A_list\n",
    "\n",
    "def recomposition_signal(A_list, lo_R, hi_R, cA, cD, f_samp, n):\n",
    "    for i in range(0,n):\n",
    "        # Intercalamento do sinal com zeros\n",
    "        scA = np.zeros((1, 2*len(cA)))[0]\n",
    "        scA = [cA[int(index/2)] if index%2==0 else 0 for index, x in enumerate(scA)]\n",
    "        scD = np.zeros((1, 2*len(cD)))[0]\n",
    "        scD = [cD[int(index/2)] if index%2==0 else 0 for index, x in enumerate(scD)]\n",
    "        \n",
    "        if len(scA) > len(A_list[n-1-i]):\n",
    "            scA = scA[0:len(scA)-1]\n",
    "            scD = scD[0:len(scD)-1]\n",
    "\n",
    "        # Aplicação do Filtro e retirada de frequência amostras no início e no final\n",
    "        cA = filtfilt(lo_R,1,scA, padlen=0)\n",
    "        cA = cA[f_samp:len(cA)-f_samp]\n",
    "\n",
    "    return cA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtendo métrica ACL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mexican_hat_filter(scale = 2):\n",
    "    step = 1/scale\n",
    "    X = np.arange(-5, 5, step)\n",
    "    np.append(X, 5)\n",
    "\n",
    "    Y = []\n",
    "    for i in range(0, len(X)):\n",
    "        Y.append(2.1741*(1/math.sqrt(2*math.pi) * (1 - X[i]**2) * np.exp(-X[i]**2/2)))\n",
    "  \n",
    "    return Y   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavelet_transform(channel_data, num):\n",
    "    gn = [-2, 2]\n",
    "    hn = [1/8, 3/8, 3/8, 1/8]\n",
    "\n",
    "    for iter in range(num):\n",
    "        wavelet_transformed_data = convolve(channel_data, gn)\n",
    "        \n",
    "        for iter_gn in range(int(len(gn)/2)):\n",
    "            wavelet_transformed_data = np.delete(wavelet_transformed_data, 0)\n",
    "            wavelet_transformed_data = np.delete(wavelet_transformed_data, len(wavelet_transformed_data) - 1)\n",
    "\n",
    "        channel_data = convolve(channel_data, hn)\n",
    "        \n",
    "        for iter_hn in range(int(len(hn)/2)):\n",
    "            channel_data = np.delete(channel_data, 0)\n",
    "            channel_data = np.delete(channel_data, len(channel_data) - 1)\n",
    "        \n",
    "        new_gn = []\n",
    "        new_hn = []\n",
    "\n",
    "        for iter_gn in range(len(gn)):\n",
    "            new_gn.append(gn[iter_gn])\n",
    "            new_gn.append(0)\n",
    "        gn = new_gn\n",
    "\n",
    "        for iter_hn in range(len(hn)):\n",
    "            new_hn.append(hn[iter_hn])\n",
    "            new_hn.append(0)\n",
    "        hn = new_hn\n",
    "\n",
    "    return wavelet_transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acl(wavelet_transformed_data, floating_window_size):\n",
    "    acl = np.zeros(len(wavelet_transformed_data))\n",
    "\n",
    "    # A métrica ACL é definida pelo produto entre a função relativa a área abaixo da onda e\n",
    "    # a função representando a curva da onda.\n",
    "\n",
    "    for iter_sample in range(len(wavelet_transformed_data) - floating_window_size):\n",
    "\n",
    "        # y_k é um vetor incluindo amostras de k até k + L da versão filtrada relativa à escala 2λ\n",
    "        y_k = wavelet_transformed_data[iter_sample : (iter_sample + floating_window_size - 1)]\n",
    "        area_k = sum(abs(y_k))\n",
    "        \n",
    "        curve_k = 0\n",
    "        for iter_yk in range(1, len(y_k)):\n",
    "            curve_k = curve_k + math.sqrt(1 + (y_k[iter_yk] - y_k[iter_yk - 1])**2)\n",
    "        \n",
    "        acl[iter_sample] = area_k*curve_k\n",
    "    \n",
    "    return acl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extração de parâmetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complexo QRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_superenergy_signal(data, f_samp, scale = 2):\n",
    "    filter = generate_mexican_hat_filter(scale)\n",
    "    window_size = round(0.15 * f_samp)\n",
    "    \n",
    "    samples_lenght = len(data)\n",
    "    result_signal = np.zeros(samples_lenght)\n",
    "\n",
    "    filtered_data = convolve(data, filter)\n",
    "\n",
    "    # Remoção dos dados não úteis resultantes da convolução\n",
    "    gap = int(np.round(len(filter) / 2)) - 1\n",
    "    filtered_data = np.copy(filtered_data[gap : (len(filtered_data) - gap)])\n",
    "    \n",
    "    derivative_filter = np.diff(filtered_data)\n",
    "    \n",
    "    envelope = hilbert(derivative_filter)\n",
    "    envelope_amplitude = np.abs(envelope)\n",
    "    \n",
    "    result_signal = result_signal + envelope_amplitude\n",
    "\n",
    "    result_signal_index = list(range(len(result_signal)))\n",
    "    result_signal = np.array([0 if (index < window_size or index > len(result_signal) - (window_size + 1)) else signal for index, signal in zip(result_signal_index, result_signal)])\n",
    "\n",
    "    return result_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_signal_peaks(data, f_samp):\n",
    "    data = data / max(abs(data))\n",
    "    sample_time = len(data) / f_samp\n",
    "\n",
    "    minimum_beats = math.floor(0.7 * sample_time)\n",
    "    maximum_beats = math.ceil(3.5 * sample_time)\n",
    "    \n",
    "    current_number_of_beats = 100\n",
    "    threshold = 0\n",
    "    while(current_number_of_beats < minimum_beats or current_number_of_beats > maximum_beats):    \n",
    "        \n",
    "        if current_number_of_beats > maximum_beats:\n",
    "            threshold = threshold + 0.1\n",
    "        elif current_number_of_beats < minimum_beats:\n",
    "            threshold = threshold - 0.01\n",
    "\n",
    "        try:\n",
    "            peaks = find_peaks(data, height = threshold, distance = 0.3 * f_samp)[0]\n",
    "            current_number_of_beats = len(peaks)\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    return peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qrs_peak(data_gap, data, data_gap_start, data_gap_end, data_start, data_end, f_samp, scale = 2):\n",
    "    superenergy_signal = get_superenergy_signal(data_gap, f_samp, scale)\n",
    "    \n",
    "    superenergy_signal_start = int(data_start - data_gap_start)\n",
    "    superenergy_signal_end = len(superenergy_signal) - int(data_gap_end - data_end)\n",
    "    superenergy_signal = superenergy_signal[superenergy_signal_start:superenergy_signal_end]\n",
    "\n",
    "    superenergy_signal_peaks = find_signal_peaks(superenergy_signal, f_samp)\n",
    "\n",
    "    qrs_peaks = []\n",
    "    for iter_peak in range(len(superenergy_signal_peaks)):\n",
    "        current_peak = superenergy_signal_peaks[iter_peak]\n",
    "\n",
    "        # Procurando o pico da onda R em uma janela de 120 ms\n",
    "        window_size = round(0.10*f_samp)\n",
    "            \n",
    "        if (current_peak - window_size) >= 0:\n",
    "            qrs_peaks.append(current_peak - window_size + np.argmax(abs(data[current_peak - window_size : current_peak + window_size])))\n",
    "        else:\n",
    "            qrs_peaks.append(np.argmax(abs(data[0 : current_peak + window_size])))\n",
    "            \n",
    "    return qrs_peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qrs_interval(qrs_peaks, acl, f_samp, delay):\n",
    "    qrs_on = []\n",
    "    qrs_off = []\n",
    "    \n",
    "    for iter_peak in range(len(qrs_peaks)):\n",
    "        \n",
    "        iter_sample_on = qrs_peaks[iter_peak]\n",
    "        if (iter_sample_on - round(0.12*f_samp) >= 1):\n",
    "            window = acl[(iter_sample_on - round(0.12*f_samp)) : (iter_sample_on + round(0.12*f_samp))]\n",
    "        else:\n",
    "            window = acl[0 : (iter_sample_on + round(0.12*f_samp))]\n",
    "        \n",
    "        found = False\n",
    "        while not(found):\n",
    "            if acl[iter_sample_on] < 1.1*acl[iter_sample_on - 1] and acl[iter_sample_on] < 1.1*acl[iter_sample_on + 1] and qrs_peaks[iter_peak] - (iter_sample_on + delay) >= 0.06*f_samp and acl[iter_sample_on] < 0.7*max(window):\n",
    "                found = True\n",
    "                break\n",
    "            else:\n",
    "                iter_sample_on = iter_sample_on - 1\n",
    "        qrs_on.append(iter_sample_on + delay)\n",
    "\n",
    "        iter_sample_end = qrs_peaks[iter_peak]\n",
    "        min_value = acl[iter_sample_end]\n",
    "        min_position = iter_sample_end\n",
    "        found = False\n",
    "        while not(found):\n",
    "            if acl[iter_sample_end] < 1.1*acl[iter_sample_end - 1] and acl[iter_sample_end] < 1.1*acl[iter_sample_end + 1] and iter_sample_end - qrs_peaks[iter_peak] >= 0.06*f_samp and acl[iter_sample_end] < 0.7*max(window):\n",
    "                found = True\n",
    "                break\n",
    "            else:\n",
    "                if acl[iter_sample_end] <= min_value:\n",
    "                    min_value = acl[iter_sample_end]\n",
    "                    min_position = iter_sample_end\n",
    "                iter_sample_end = iter_sample_end + 1\n",
    "                \n",
    "                if iter_sample_end >= len(acl) - 1:\n",
    "                    iter_sample_end = min_position\n",
    "                    found = True\n",
    "                    break\n",
    "        qrs_off.append(iter_sample_end)\n",
    "\n",
    "    return qrs_on, qrs_off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qrs_amplitudes(qrs_peaks, qrs_on, data):\n",
    "    qrs_amplitudes = [float(\"{:.4f}\".format((data[peak] - data[start]))) for peak, start in zip(qrs_peaks, qrs_on)]\n",
    "    \n",
    "    return qrs_amplitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rr_interval(qrs_peaks, f_samp):\n",
    "    rr_intervals = []\n",
    "    for iter_peak in range(1, len(qrs_peaks)):\n",
    "        rr_intervals.append((qrs_peaks[iter_peak] - qrs_peaks[iter_peak - 1]) / f_samp)\n",
    "    \n",
    "    return rr_intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Onda T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_t_interval(median_qrs_on, median_qrs_off, delay):\n",
    "    search_t_on = []\n",
    "    search_t_on.extend(median_qrs_off)\n",
    "    search_t_on.pop()\n",
    "\n",
    "    search_t_off = []\n",
    "    search_t_off.extend(median_qrs_on)\n",
    "    search_t_off.pop(0)\n",
    "\n",
    "    for iter in range(len(search_t_off)):\n",
    "        search_t_off[iter] = search_t_off[iter] - delay\n",
    "        search_t_off[iter] = int(search_t_on[iter] + (search_t_off[iter] - search_t_on[iter]) / 2)\n",
    "\n",
    "    return search_t_on, search_t_off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_t_peak(data, acl, t_on, t_off, delay):\n",
    "    t_peaks = []\n",
    "    current_channel_t_peaks = []\n",
    "    current_acl = acl\n",
    "\n",
    "    for iter in range(len(t_on)):\n",
    "        search_area = current_acl[t_on[iter] : t_off[iter]]\n",
    "        try:\n",
    "            current_t_peak = np.where(search_area == np.amax(search_area))[0][0]\n",
    "        except:\n",
    "            continue\n",
    "        current_channel_t_peaks.append(current_t_peak + t_on[iter])\n",
    "\n",
    "    current_channel_real_t_peaks = []\n",
    "    current_channel_data = np.copy(data)\n",
    "    for iter_peak in range(len(current_channel_t_peaks)):\n",
    "        current_peak = current_channel_t_peaks[iter_peak]\n",
    "\n",
    "        if (current_peak - delay) >= 0:\n",
    "            if current_channel_data[current_peak] >= 0:\n",
    "                current_channel_real_t_peaks.append(current_peak - delay + np.argmax(current_channel_data[current_peak - delay : current_peak + delay]))\n",
    "            else:\n",
    "                current_channel_real_t_peaks.append(current_peak - delay + np.argmin(current_channel_data[current_peak - delay : current_peak + delay]))\n",
    "        else:\n",
    "            if current_channel_data[current_peak] >= 0:\n",
    "                current_channel_real_t_peaks.append(np.argmax(current_channel_data[0 : current_peak + delay]))\n",
    "            else:\n",
    "                current_channel_real_t_peaks.append(np.argmin(current_channel_data[0 : current_peak + delay]))\n",
    "\n",
    "    t_peaks = (current_channel_real_t_peaks)\n",
    "\n",
    "    return t_peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_t_interval(t_peaks, qrs_off, acl, f_samp, delay):\n",
    "    t_on = []\n",
    "    t_off = []\n",
    "\n",
    "    for iter_peak in range(len(t_peaks)):\n",
    "\n",
    "        iter_sample_on = t_peaks[iter_peak]\n",
    "        if (iter_sample_on - round(0.12*f_samp) >= 0):\n",
    "            window = acl[(iter_sample_on - round(0.12*f_samp)) : (iter_sample_on + round(0.12*f_samp))]\n",
    "        else:\n",
    "            window = acl[0 : (iter_sample_on + round(0.12*f_samp))]\n",
    "\n",
    "        found = False\n",
    "        while not(found):\n",
    "            if iter_sample_on <= qrs_off[iter_peak]:\n",
    "                found = True\n",
    "                break\n",
    "            if acl[iter_sample_on] < 1.1*acl[iter_sample_on - 1] and acl[iter_sample_on] < 1.1*acl[iter_sample_on + 1] and t_peaks[iter_peak] - (iter_sample_on + delay) >= 0.1*f_samp and acl[iter_sample_on] < 0.7*max(window):\n",
    "                found = True\n",
    "                break\n",
    "            else:\n",
    "                iter_sample_on = iter_sample_on - 1\n",
    "        t_on.append(iter_sample_on)\n",
    "\n",
    "        iter_sample_end = t_peaks[iter_peak]\n",
    "        min_value = acl[iter_sample_end]\n",
    "        min_position = iter_sample_end\n",
    "        found = False\n",
    "        while not(found):\n",
    "            if acl[iter_sample_end] < 1.1*acl[iter_sample_end - 1] and acl[iter_sample_end] < 1.1*acl[iter_sample_end + 1] and iter_sample_end - t_peaks[iter_peak] >= 0.1*f_samp and acl[iter_sample_end] < 0.7*max(window):\n",
    "                found = True\n",
    "                break\n",
    "            else:\n",
    "                if acl[iter_sample_end] <= min_value:\n",
    "                    min_value = acl[iter_sample_end]\n",
    "                    min_position = iter_sample_end\n",
    "                iter_sample_end = iter_sample_end + 1\n",
    "\n",
    "                if iter_sample_end >= len(acl) - 1:\n",
    "                    iter_sample_end = min_position\n",
    "                    found = True\n",
    "                    break\n",
    "        t_off.append(iter_sample_end)\n",
    "\n",
    "    return t_on, t_off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_t_amplitudes(t_peaks, median_t_off, data):\n",
    "    t_amplitudes = []\n",
    "\n",
    "    current_difference = [float(\"{:.4f}\".format((data[peak] - data[end]))) for peak, end in zip(t_peaks, median_t_off)]\n",
    "    t_amplitudes.extend(current_difference)\n",
    "\n",
    "    return t_amplitudes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Onda P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_p_interval(median_qrs_on, median_qrs_off, delay):\n",
    "    search_p_on = []\n",
    "    search_p_on.extend(median_qrs_off)\n",
    "    search_p_on.pop()\n",
    "\n",
    "    search_p_off = []\n",
    "    search_p_off.extend(median_qrs_on)\n",
    "    search_p_off.pop(0)\n",
    "\n",
    "    for iter in range(len(search_p_off)):\n",
    "        search_p_off[iter] = search_p_off[iter] - delay\n",
    "        search_p_on[iter] = int(search_p_on[iter] + (search_p_off[iter] - search_p_on[iter]) / 2)\n",
    "\n",
    "    return search_p_on, search_p_off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p_peak(data, acl, p_on, p_off, delay):\n",
    "    p_peaks = []\n",
    "\n",
    "    current_channel_p_peaks = []\n",
    "    current_acl = acl\n",
    "\n",
    "    for iter in range(len(p_on)):\n",
    "        search_area = current_acl[p_on[iter] : p_off[iter]]\n",
    "        try:\n",
    "            current_p_peak = np.where(search_area == np.amax(search_area))[0][0]\n",
    "        except:\n",
    "            continue\n",
    "        current_channel_p_peaks.append(current_p_peak + p_on[iter])\n",
    "\n",
    "    current_channel_real_p_peaks = []\n",
    "    current_channel_data = np.copy(data)\n",
    "    for iter_peak in range(len(current_channel_p_peaks)):\n",
    "        current_peak = current_channel_p_peaks[iter_peak]\n",
    "\n",
    "        if (current_peak - delay) >= 0:\n",
    "            if current_channel_data[current_peak] >= 0:\n",
    "                current_channel_real_p_peaks.append(current_peak - delay + np.argmax(current_channel_data[current_peak - delay : current_peak + delay]))\n",
    "            else:\n",
    "                current_channel_real_p_peaks.append(current_peak - delay + np.argmin(current_channel_data[current_peak - delay : current_peak + delay]))\n",
    "        else:\n",
    "            if current_channel_data[current_peak] >= 0:\n",
    "                current_channel_real_p_peaks.append(np.argmax(current_channel_data[0 : current_peak + delay]))\n",
    "            else:\n",
    "                current_channel_real_p_peaks.append(np.argmin(current_channel_data[0 : current_peak + delay]))\n",
    "\n",
    "        p_peaks = (current_channel_real_p_peaks)\n",
    "\n",
    "    return p_peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p_interval(p_peaks, qrs_on, acl, f_samp, delay):\n",
    "    p_on = []\n",
    "    p_off = []\n",
    "\n",
    "    for iter_peak in range(len(p_peaks)):\n",
    "\n",
    "        iter_sample_on = p_peaks[iter_peak]\n",
    "        if (iter_sample_on - round(0.12*f_samp) >= 0):\n",
    "            window = acl[(iter_sample_on - round(0.12*f_samp)) : (iter_sample_on + round(0.12*f_samp))]\n",
    "        else:\n",
    "            window = acl[0 : (iter_sample_on + round(0.12*f_samp))]\n",
    "\n",
    "        found = False\n",
    "        while not(found):\n",
    "            if acl[iter_sample_on] < 1.2*acl[iter_sample_on - 1] and acl[iter_sample_on] < 1.2*acl[iter_sample_on + 1] and p_peaks[iter_peak] - (iter_sample_on + delay) >= 0.04*f_samp and acl[iter_sample_on] < 0.7*max(window):\n",
    "                found = True\n",
    "                break\n",
    "            else:\n",
    "                iter_sample_on = iter_sample_on - 1\n",
    "\n",
    "            if iter_sample_on <= 0:\n",
    "                iter_sample_on = - delay\n",
    "                found = True\n",
    "                break\n",
    "        p_on.append(iter_sample_on + delay)\n",
    "\n",
    "        iter_sample_end = p_peaks[iter_peak]\n",
    "        min_value = acl[iter_sample_end]\n",
    "        min_position = iter_sample_end\n",
    "        found = False\n",
    "        while not(found):\n",
    "            if iter_sample_end >= qrs_on[iter_peak + 1]:\n",
    "                found = True\n",
    "                break\n",
    "            if acl[iter_sample_end] < 1.2*acl[iter_sample_end - 1] and acl[iter_sample_end] < 1.2*acl[iter_sample_end + 1] and iter_sample_end - p_peaks[iter_peak] >= 0.04*f_samp and acl[iter_sample_end] < 0.7*max(window):\n",
    "                found = True\n",
    "                break\n",
    "            else:\n",
    "                if acl[iter_sample_end] <= min_value:\n",
    "                    min_value = acl[iter_sample_end]\n",
    "                    min_position = iter_sample_end\n",
    "                iter_sample_end = iter_sample_end + 1\n",
    "\n",
    "                if iter_sample_end >= len(acl) - 1:\n",
    "                    iter_sample_end = min_position\n",
    "                    found = True\n",
    "                    break\n",
    "        p_off.append(iter_sample_end)\n",
    "\n",
    "    return p_on, p_off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p_amplitudes(p_peaks, median_qrs_on, data):\n",
    "    p_amplitudes = []\n",
    "    median_qrs_on = median_qrs_on[1:len(median_qrs_on)]\n",
    "    current_difference = [float(\"{:.4f}\".format((data[peak] - data[start]))) for peak, start in zip(p_peaks, median_qrs_on)]\n",
    "    p_amplitudes.extend(current_difference)\n",
    "\n",
    "    return p_amplitudes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rr_interval(qrs_peaks, f_samp):\n",
    "\n",
    "    rr_intervals = []\n",
    "    for iter_peak in range(1, len(qrs_peaks)):\n",
    "        rr_intervals.append((qrs_peaks[iter_peak] - qrs_peaks[iter_peak-1]) / f_samp)\n",
    "\n",
    "    return rr_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pr_interval(median_p_on, median_qrs_on, f_samp):\n",
    "    pr_on = []\n",
    "    pr_off = []\n",
    "    pr_interval = []\n",
    "\n",
    "    for iter_sample in range(len(median_p_on)):\n",
    "        pr_on.append(median_p_on[iter_sample])\n",
    "        pr_off.append(median_qrs_on[iter_sample + 1])\n",
    "        pr_interval.append((pr_off[iter_sample] - pr_on[iter_sample])/f_samp)\n",
    "\n",
    "    return pr_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qt_interval(median_qrs_on, median_t_off, f_samp):\n",
    "    qt_on = []\n",
    "    qt_off = []\n",
    "    qt_interval = []\n",
    "\n",
    "    for iter_sample in range(len(median_t_off)):\n",
    "        qt_on.append(median_qrs_on[iter_sample])\n",
    "        qt_off.append(median_t_off[iter_sample])\n",
    "        qt_interval.append((qt_off[iter_sample] - qt_on[iter_sample])/f_samp)\n",
    "\n",
    "    return qt_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qtc_interval(qt_interval, beat_interval_mean):\n",
    "    qtc_interval = []\n",
    "\n",
    "    for interval in qt_interval:\n",
    "        try:\n",
    "            qtc_interval.append(interval / (np.sqrt(beat_interval_mean)))\n",
    "        except:\n",
    "            qtc_interval.append(None)\n",
    "\n",
    "    return qtc_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_st_deviation(median_qrs_on, median_qrs_off, data):\n",
    "    st_deviation = []\n",
    "    st_on = median_qrs_off[0 : len(median_qrs_off) - 1]\n",
    "    st_off = median_qrs_on[1 : len(median_qrs_on)]\n",
    "\n",
    "    current_amplitude = [float(\"{:.4f}\".format((data[on] - data[off]))) for on, off in zip(st_on, st_off)]\n",
    "    st_deviation.extend(current_amplitude)\n",
    "    \n",
    "    return st_deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pr_segment(median_p_off, median_qrs_on, f_samp):\n",
    "    qrs_on = []\n",
    "    pr_off = []\n",
    "    pr_segment = []\n",
    "\n",
    "    for iter_sample in range(len(median_p_off)):\n",
    "        qrs_on.append(median_p_off[iter_sample])\n",
    "        pr_off.append(median_qrs_on[iter_sample])\n",
    "        pr_segment.append((qrs_on[iter_sample] - pr_off[iter_sample])/f_samp)\n",
    "\n",
    "    return pr_segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_heart_rate(rr_interval):\n",
    "    heart_rate = []\n",
    "\n",
    "    for interval in rr_interval:\n",
    "        heart_rate.append(60/interval)\n",
    "\n",
    "    return heart_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Função Principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ecg_info(data, f_samp):\n",
    "    acl = []\n",
    "    ecg_extracted_data = {}\n",
    "    ecg_extracted_data[f'ecg_duration'] = 0\n",
    "    ecg_extracted_data[f'number_of_beats'] = 0\n",
    "    ecg_extracted_data[f'qrs_amplitude'] = []\n",
    "    ecg_extracted_data[f't_amplitude'] = []\n",
    "    ecg_extracted_data[f'p_amplitude'] = []\n",
    "    ecg_extracted_data[f'qrs_peaks'] = []\n",
    "    ecg_extracted_data[f'qrs_on'] = []\n",
    "    ecg_extracted_data[f'qrs_off'] = []\n",
    "    ecg_extracted_data[f't_peaks'] = []\n",
    "    ecg_extracted_data[f't_on'] = []\n",
    "    ecg_extracted_data[f't_off'] = []\n",
    "    ecg_extracted_data[f'p_peaks'] = []\n",
    "    ecg_extracted_data[f'p_on'] = []\n",
    "    ecg_extracted_data[f'p_off'] = []\n",
    "    ecg_extracted_data[f'rr_interval'] = []\n",
    "    ecg_extracted_data[f'pr_interval'] = []\n",
    "    ecg_extracted_data[f'qt_interval'] = []\n",
    "    ecg_extracted_data[f'pr_segment'] = []\n",
    "    ecg_extracted_data[f'st_deviation'] = []\n",
    "\n",
    "    # Aplicação do Filtro de Linha de Base\n",
    "    data = baseline_filter(data, f_samp, 8)\n",
    "    ecg_extracted_data[f'filtered_data'] = data\n",
    "\n",
    "    floating_window_size = round(0.04 * f_samp)\n",
    "\n",
    "    for segment in range(0, math.ceil(len(data)/(10*f_samp))):\n",
    "        # Segmento de 10 segundos\n",
    "        start_segment = int(segment*(10*f_samp))\n",
    "        end_segment = int((segment+1)*(10*f_samp))\n",
    "        if end_segment <= len(data-1):\n",
    "            data_segment = data[start_segment:end_segment]\n",
    "        else:\n",
    "            data_segment = len(data)\n",
    "            data_segment = data[start_segment:]\n",
    "        \n",
    "        # Segmento com gap de 0.5 segundos\n",
    "        start_gap_segment = start_segment - int(0.5*f_samp)\n",
    "        end_gap_segment = end_segment + int(0.5*f_samp)\n",
    "\n",
    "        if start_gap_segment < 0:\n",
    "            start_gap_segment = 0\n",
    "\n",
    "        if end_gap_segment <= len(data-1):\n",
    "            data_gap_segment = data[start_gap_segment:end_gap_segment]\n",
    "        else:\n",
    "            end_gap_segment = len(data)\n",
    "            data_gap_segment = data[start_gap_segment:]\n",
    "\n",
    "        ecg_extracted_data[f'ecg_duration'] += round((len(data_segment)/f_samp), 4)\n",
    "            \n",
    "        qrs_peaks = get_qrs_peak(data_gap_segment, data_segment, start_gap_segment, end_gap_segment, start_segment, end_segment, f_samp)\n",
    "        ecg_extracted_data[f'qrs_peaks'].extend([peak + (segment*10*f_samp) for peak in qrs_peaks])\n",
    "        ecg_extracted_data[f'number_of_beats'] += len(qrs_peaks)\n",
    "\n",
    "        # Intervalos do complexo QRS\n",
    "        wavelet_transformed_data = wavelet_transform(data_gap_segment, 3)\n",
    "        acl = get_acl(wavelet_transformed_data, floating_window_size)\n",
    "        \n",
    "        delay = floating_window_size\n",
    "        qrs_on, qrs_off = get_qrs_interval(qrs_peaks, acl, f_samp, delay)\n",
    "        ecg_extracted_data[f'qrs_on'].extend([on + (segment*10*f_samp) for on in qrs_on])\n",
    "        ecg_extracted_data[f'qrs_off'].extend([off + (segment*10*f_samp) for off in qrs_off])\n",
    "\n",
    "        # Amplitudes do complexo QRS\n",
    "        qrs_amplitudes = get_qrs_amplitudes(qrs_peaks, qrs_on, data_gap_segment)\n",
    "        ecg_extracted_data[f'qrs_amplitude'].extend(qrs_amplitudes)\n",
    "\n",
    "        # Intervalo de Busca Onda T\n",
    "        search_t_on, search_t_off = search_t_interval(qrs_on, qrs_off, delay)\n",
    "\n",
    "        # # Picos Onda T\n",
    "        t_peaks = get_t_peak(data_segment, acl, search_t_on, search_t_off, delay)\n",
    "        ecg_extracted_data[f't_peaks'].extend([peak + (segment*10*f_samp) for peak in t_peaks])\n",
    "\n",
    "        # Intervalos Onda T\n",
    "        t_on, t_off = get_t_interval(t_peaks, qrs_off, acl, f_samp, delay)\n",
    "        t_intervals = []\n",
    "        t_intervals.append((t_on, t_off))\n",
    "        ecg_extracted_data[f't_on'].extend([on + (segment*10*f_samp) for on in t_on])\n",
    "        ecg_extracted_data[f't_off'].extend([off + (segment*10*f_samp) for off in t_off])\n",
    "\n",
    "        # Amplitudes Onda T\n",
    "        t_amplitudes = get_t_amplitudes(t_peaks, t_off, data)\n",
    "        ecg_extracted_data[f't_amplitude'].extend(t_amplitudes)\n",
    "\n",
    "        # Intervalo de Busca Onda P\n",
    "        search_p_on, search_p_off = search_p_interval(qrs_on, qrs_off, delay)\n",
    "\n",
    "        # Picos Onda P\n",
    "        p_peaks = get_p_peak(data_segment, acl, search_p_on, search_p_off, delay)\n",
    "        ecg_extracted_data[f'p_peaks'].extend([peak + (segment*10*f_samp) for peak in p_peaks])\n",
    "\n",
    "        # Intervalos Onda P\n",
    "        p_on, p_off = get_p_interval(p_peaks, qrs_off, acl, f_samp, delay)\n",
    "        p_intervals = []\n",
    "        p_intervals.append((p_on, p_off))\n",
    "        ecg_extracted_data[f'p_on'].extend([on + (segment*10*f_samp) for on in p_on])\n",
    "        ecg_extracted_data[f'p_off'].extend([off + (segment*10*f_samp) for off in p_off])\n",
    "\n",
    "        # Amplitudes Onda P\n",
    "        p_amplitudes = get_p_amplitudes(t_peaks, p_off, data_segment)\n",
    "        ecg_extracted_data[f'p_amplitude'].extend(p_amplitudes)\n",
    "\n",
    "    # Intervalo RR\n",
    "    rr_interval = get_rr_interval(ecg_extracted_data[f'qrs_peaks'], f_samp)\n",
    "    ecg_extracted_data[f'rr_interval'].extend(rr_interval)\n",
    "\n",
    "    # Intervalo PR\n",
    "    pr_interval = get_pr_interval(ecg_extracted_data[f'p_on'], ecg_extracted_data[f'qrs_on'], f_samp)\n",
    "    ecg_extracted_data[f'pr_interval'].extend(pr_interval)\n",
    "\n",
    "    # Intervalo QT\n",
    "    qt_interval = get_qt_interval(ecg_extracted_data[f'qrs_on'], ecg_extracted_data[f't_off'], f_samp)\n",
    "    ecg_extracted_data[f'qt_interval'].extend(qt_interval)\n",
    "\n",
    "    # Segmento PR \n",
    "    pr_segment = get_pr_segment(ecg_extracted_data[f'p_off'], ecg_extracted_data[f'qrs_on'], f_samp)\n",
    "    ecg_extracted_data[f'pr_segment'].extend(pr_segment)\n",
    "\n",
    "    # Derivação ST\n",
    "    st_deviation = get_st_deviation(ecg_extracted_data[f'qrs_on'], ecg_extracted_data[f'qrs_off'], data)\n",
    "    ecg_extracted_data[f'st_deviation'].extend(st_deviation)\n",
    "    \n",
    "    # Média das Amplitudes do QRS\n",
    "    scaler = Normalizer()\n",
    "    standardized_qrs_amplitude = np.concatenate(scaler.fit_transform(np.array(ecg_extracted_data[f'qrs_amplitude']).reshape(-1,1)), axis=0)\n",
    "    try:\n",
    "        ecg_extracted_data[f'mean_qrs_amplitude'] = round(statistics.mean(standardized_qrs_amplitude), 4)\n",
    "    except:\n",
    "        ecg_extracted_data[f'mean_qrs_amplitude'] = None\n",
    "\n",
    "    # Desvio Padrão das Amplitudes do QRS\n",
    "    try:\n",
    "        ecg_extracted_data[f'standard_deviation_qrs_amplitude'] = round(statistics.stdev(standardized_qrs_amplitude), 4)\n",
    "    except:\n",
    "        ecg_extracted_data[f'standard_deviation_qrs_amplitude'] = None\n",
    "\n",
    "    # Média das Amplitudes da Onda T\n",
    "    scaler = Normalizer()\n",
    "    standardized_t_amplitude = np.concatenate(scaler.fit_transform(np.array(ecg_extracted_data[f't_amplitude']).reshape(-1,1)), axis=0)\n",
    "    try:\n",
    "        ecg_extracted_data[f'mean_t_amplitude'] = round(statistics.mean(standardized_t_amplitude), 4)\n",
    "    except:\n",
    "        ecg_extracted_data[f'mean_t_amplitude'] = None\n",
    "\n",
    "    # Desvio Padrão das Amplitudes da Onda T\n",
    "    try:\n",
    "        ecg_extracted_data[f'standard_deviation_t_amplitude'] = round(statistics.stdev(standardized_t_amplitude), 4)\n",
    "    except:\n",
    "        ecg_extracted_data[f'standard_deviation_t_amplitude'] = None\n",
    "\n",
    "    # Média das Amplitudes da Onda T\n",
    "    scaler = Normalizer()\n",
    "    standardized_p_amplitude = np.concatenate(scaler.fit_transform(np.array(ecg_extracted_data[f'p_amplitude']).reshape(-1,1)), axis=0)\n",
    "    try:\n",
    "        ecg_extracted_data[f'mean_p_amplitude'] = round(statistics.mean(standardized_p_amplitude), 4)\n",
    "    except:\n",
    "        ecg_extracted_data[f'mean_p_amplitude'] = None\n",
    "\n",
    "    # Desvio Padrão das Amplitudes da Onda T\n",
    "    try:\n",
    "        ecg_extracted_data[f'standard_deviation_p_amplitude'] = round(statistics.stdev(standardized_p_amplitude), 4)\n",
    "    except:\n",
    "        ecg_extracted_data[f'standard_deviation_p_amplitude'] = None\n",
    "\n",
    "    # Média dos Intervalos RR\n",
    "    try:\n",
    "        ecg_extracted_data[f'mean_rr_interval'] = round(statistics.mean(ecg_extracted_data[f'rr_interval']), 4)\n",
    "    except:\n",
    "        ecg_extracted_data[f'mean_rr_interval'] = None\n",
    "\n",
    "    # Desvio Padrão dos Intervalos RR\n",
    "    try:\n",
    "        ecg_extracted_data[f'standard_deviation_rr_interval'] = round(statistics.stdev(ecg_extracted_data[f'rr_interval']), 4)\n",
    "    except:\n",
    "        ecg_extracted_data[f'standard_deviation_rr_interval'] = None\n",
    "\n",
    "    # Média e Desvião Padrão Intervalo PR\n",
    "    try:\n",
    "        ecg_extracted_data[f'mean_pr_interval'] = round(statistics.mean(ecg_extracted_data[f'pr_interval']), 4)\n",
    "    except:\n",
    "        ecg_extracted_data[f'mean_pr_interval'] = None\n",
    "    try:\n",
    "        ecg_extracted_data[f'standard_deviation_pr_interval'] = round(statistics.stdev(ecg_extracted_data[f'pr_interval']), 4)\n",
    "    except:\n",
    "        ecg_extracted_data[f'standard_deviation_pr_interval'] = None\n",
    "\n",
    "    # Média e Desvião Padrão Intervalo QT\n",
    "    try:\n",
    "        ecg_extracted_data[f'mean_qt_interval'] = round(statistics.mean(ecg_extracted_data[f'qt_interval']), 4)\n",
    "    except:\n",
    "        ecg_extracted_data[f'mean_qt_interval'] = None\n",
    "    try:\n",
    "        ecg_extracted_data[f'standard_deviation_qt_interval'] = round(statistics.stdev(ecg_extracted_data[f'qt_interval']), 4)\n",
    "    except:\n",
    "        ecg_extracted_data[f'standard_deviation_qt_interval'] = None\n",
    "\n",
    "    # Média e Desvião Padrão Segmento PR\n",
    "    try:\n",
    "        ecg_extracted_data[f'mean_pr_segment'] = round(statistics.mean(ecg_extracted_data[f'pr_segment']), 4)\n",
    "    except:\n",
    "        ecg_extracted_data[f'mean_pr_segment'] = None\n",
    "    try:\n",
    "        ecg_extracted_data[f'standard_deviation_pr_segment'] = round(statistics.stdev(ecg_extracted_data[f'pr_segment']), 4)\n",
    "    except:\n",
    "        ecg_extracted_data[f'standard_deviation_pr_segment'] = None\n",
    "\n",
    "    # Média da Derivação ST\n",
    "    scaler = Normalizer()\n",
    "    standardized_st_deviation = np.concatenate(scaler.fit_transform(np.array(ecg_extracted_data[f'st_deviation']).reshape(-1,1)), axis=0)\n",
    "    try:\n",
    "        ecg_extracted_data[f'mean_st_deviation'] = round(statistics.mean(standardized_st_deviation), 4)\n",
    "    except:\n",
    "        ecg_extracted_data[f'mean_st_deviation'] = None\n",
    "\n",
    "    # Desvio Padrão da Derivação ST\n",
    "    try:\n",
    "        ecg_extracted_data[f'standard_deviation_st_deviation'] = round(statistics.stdev(standardized_st_deviation), 4)\n",
    "    except:\n",
    "        ecg_extracted_data[f'standard_deviation_st_deviation'] = None\n",
    "\n",
    "    # Média e Desvio Padrão da Frequência Cardíaca\n",
    "    heart_rate = get_heart_rate(rr_interval)\n",
    "    try:\n",
    "        ecg_extracted_data[f'mean_heart_rate'] = round(statistics.mean(heart_rate), 4)\n",
    "    except:\n",
    "        ecg_extracted_data[f'mean_heart_rate'] = None\n",
    "    try:\n",
    "        ecg_extracted_data[f'standard_deviation_heart_rate'] = round(statistics.stdev(heart_rate), 4)\n",
    "    except:\n",
    "        ecg_extracted_data[f'standard_deviation_heart_rate'] = None\n",
    "    \n",
    "    return ecg_extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_ecg(data, qrs_peaks, qrs_on, qrs_off, sample):\n",
    "    fig = px.line(data)\n",
    "\n",
    "    trace_qrs_peaks = go.Scatter(\n",
    "        x = qrs_peaks, \n",
    "        y = [data[i] for i in qrs_peaks], \n",
    "        name = 'R Peaks', \n",
    "        mode = 'markers', \n",
    "        marker = dict(color = 'red')\n",
    "    )\n",
    "\n",
    "    trace_qrs_on = go.Scatter(\n",
    "        x = qrs_on, \n",
    "        y = [data[i] for i in qrs_on], \n",
    "        name = 'QRS On', \n",
    "        mode = 'markers',\n",
    "        marker = dict(color = 'red')\n",
    "    )\n",
    "\n",
    "    trace_qrs_off = go.Scatter(\n",
    "        x = qrs_off, \n",
    "        y = [data[i] for i in qrs_off], \n",
    "        name = 'QRS Off', \n",
    "        mode = 'markers', \n",
    "        marker = dict(color = 'red')\n",
    "    )\n",
    "\n",
    "    fig.add_traces(trace_qrs_peaks)\n",
    "    fig.add_traces(trace_qrs_on)\n",
    "    fig.add_traces(trace_qrs_off)\n",
    "\n",
    "    fig.update_layout(\n",
    "        title = f'Amostra {sample + 1}', \n",
    "        xaxis_title = 'Índice', \n",
    "        yaxis_title = 'Amplitude'\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dict(start_dict):\n",
    "    created_dict = {}\n",
    "    created_dict['normal'] = []\n",
    "    \n",
    "    for key in start_dict.keys():\n",
    "        created_dict[key] = []\n",
    "    return created_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_sample_to_dict(all_samples_dict, ecg_extracted_data):\n",
    "    for key in ecg_extracted_data.keys():\n",
    "        try:\n",
    "            all_samples_dict[key].append(ecg_extracted_data[key])\n",
    "        except Exception as e:\n",
    "            all_samples_dict[key] = []\n",
    "            for i in range(0, len(all_samples_dict['ecg_duration_1']) - 1):\n",
    "                all_samples_dict[key].append(None)\n",
    "            all_samples_dict[key].append(ecg_extracted_data[key])\n",
    "    \n",
    "    return all_samples_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('RECORDS-normal.txt') as normal_records:\n",
    "    all_normal_samples = normal_records.readlines()\n",
    "    all_normal_samples = [record.split('/')[1] for record in all_normal_samples]\n",
    "    all_normal_samples = [record.strip() for record in all_normal_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execução e Transformação em CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_samples_dict = {}\n",
    "for iter_sample in range(len(ECG_data)):\n",
    "    print(f'Extraindo os dados da amostra {iter_sample + 1}')\n",
    "    current_data = ECG_data[iter_sample]\n",
    "    current_sample = mat_files[iter_sample].split('.')[0]\n",
    "\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        ecg_extracted_data = get_ecg_info(current_data, 300)\n",
    "        \n",
    "        if not all_samples_dict:\n",
    "            all_samples_dict = create_dict(ecg_extracted_data)\n",
    "\n",
    "        if current_sample in all_normal_samples:\n",
    "            ecg_extracted_data['normal'] = 1\n",
    "        else:\n",
    "            ecg_extracted_data['normal'] = 0\n",
    "\n",
    "        all_samples_dict = append_sample_to_dict(all_samples_dict, ecg_extracted_data)\n",
    "\n",
    "        end_time = time.time()\n",
    "        print(f'Dados da amostra {iter_sample + 1} extraídos com sucesso.\\nTempo decorrido: {end_time - start_time}\\n')\n",
    "\n",
    "    except Exception as error:\n",
    "            print(f'Não foi possível extrair os dados da amostra {iter_sample + 1}.\\n')\n",
    "            print('Erro: ', error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_samples_dataframe = pd.DataFrame.from_dict(all_samples_dict)\n",
    "all_samples_dataframe = all_samples_dataframe.drop(columns=['qrs_amplitude', 'qrs_peaks', 'qrs_on', 'qrs_off', 'rr_interval', 'filtered_data', 't_peaks', 't_on', 't_off', 'p_peaks', 'p_on', 'p_off', 'pr_interval', 'qt_interval', 'pr_segment', 'st_deviation', 't_amplitude', 'p_amplitude'])\n",
    "all_samples_dataframe.to_csv('sample_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0cd03cf433fe74c5ff79a0df37a23f503833a1ded0c87867d0b2b3c672f79493"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
